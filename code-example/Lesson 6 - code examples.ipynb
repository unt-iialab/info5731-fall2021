{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Entity feature extraction using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a sentence\n",
    "# Output: a list of entities with different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp38-cp38-win_amd64.whl (12.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Using cached blis-0.7.4-cp38-cp38-win_amd64.whl (6.5 MB)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.5-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.5-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.5-cp38-cp38-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Installing collected packages: pydantic, blis, murmurhash, catalogue, srsly, cymem, preshed, wasabi, thinc, typer, pathy, spacy-legacy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cymem-2.0.5 murmurhash-1.0.5 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.50.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.25.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-29 13:07:03.236236: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-09-29 13:07:03.236533: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "entity_types = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW',\n",
    "                'LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "\n",
    "entity_name = ['person','Nationality','Building','Institution','country','location','PRODUCT','EVENT','Title','LAW',\n",
    "                'LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "\n",
    "def entity_extraction(sentence):\n",
    "    entity_list = []\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        entity_list.append([ent.text, ent.start_char, ent.end_char, ent.label_])\n",
    "    \n",
    "    return entity_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple', 0, 5, 'ORG'], ['U.K.', 27, 31, 'GPE'], ['$1 billion', 44, 54, 'MONEY']]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "entity_list = entity_extraction(sentence)\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Bag of Words and TF-IDF features extraction\n",
    "\n",
    "Please see the following Tutorial\n",
    "\n",
    "http://dsspace.wzb.eu/pyug/text_proc_feature_extraction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Linguistic feature extraction\n",
    "\n",
    "\n",
    "Please see the following Tutorial on Linguistic Feature Extraction from Text\n",
    "\n",
    "https://github.com/victoria-ianeva/NERA-Tutorial-on-linguistic-feature-extraction/blob/master/Linguistic%20Features%20NBME%20Items%20NERA%20v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 discourse_and_pragmatic_feature\n",
    "\n",
    "def num_discourse_markers(stanza_doc):\n",
    "    # Ref: https://universaldependencies.org/docsv1/u/dep/all.html#al-u-dep/discourse\n",
    "    \"\"\"Returns the number of discourse markers\n",
    "        Args:\n",
    "            stanza_doc (obj): The stanza document object\n",
    "        Returns\n",
    "            (int): the number of discourse markers\n",
    "    \"\"\"\n",
    "    return len([1 for word in stanza_doc.sentences[0].words if word.deprel == 'discourse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blabla\n",
      "  Downloading blabla-0.2.2.tar.gz (23 kB)\n",
      "Collecting stanza==1.0.0\n",
      "  Downloading stanza-1.0.0-py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: flask==1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from blabla) (1.1.2)\n",
      "Collecting jsonpickle==1.4\n",
      "  Downloading jsonpickle-1.4-py2.py3-none-any.whl (36 kB)\n",
      "Collecting anytree==2.8.0\n",
      "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: nltk==3.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from blabla) (3.5)\n",
      "Collecting ipython==7.13.0\n",
      "  Downloading ipython-7.13.0-py3-none-any.whl (780 kB)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from blabla) (3.2.0)\n",
      "Requirement already satisfied: pyyaml==5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from blabla) (5.3.1)\n",
      "Collecting pandas==1.0.3\n",
      "  Downloading pandas-1.0.3-cp38-cp38-win_amd64.whl (8.9 MB)\n",
      "Collecting tqdm==4.46.0\n",
      "  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.0.0->blabla) (1.7.1)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.0.0->blabla) (3.17.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.0.0->blabla) (1.19.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.0.0->blabla) (2.24.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask==1.1.2->blabla) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask==1.1.2->blabla) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask==1.1.2->blabla) (7.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask==1.1.2->blabla) (2.11.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpickle==1.4->blabla) (2.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from anytree==2.8.0->blabla) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk==3.5->blabla) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk==3.5->blabla) (2020.10.15)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (5.0.5)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (3.0.8)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (0.17.1)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (4.4.2)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (2.7.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.13.0->blabla) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema==3.2.0->blabla) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema==3.2.0->blabla) (20.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==1.0.3->blabla) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==1.0.3->blabla) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.2.0->stanza==1.0.0->blabla) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza==1.0.0->blabla) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza==1.0.0->blabla) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza==1.0.0->blabla) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->stanza==1.0.0->blabla) (1.25.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask==1.1.2->blabla) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata->jsonpickle==1.4->blabla) (3.4.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython==7.13.0->blabla) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.13.0->blabla) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython==7.13.0->blabla) (0.7.0)\n",
      "Building wheels for collected packages: blabla\n",
      "  Building wheel for blabla (setup.py): started\n",
      "  Building wheel for blabla (setup.py): finished with status 'done'\n",
      "  Created wheel for blabla: filename=blabla-0.2.2-py3-none-any.whl size=30905 sha256=f6b0d54c8ca93cd7bb9a706d7797d0a586ac2bf6ae7ee7173672dd593707ccf3\n",
      "  Stored in directory: c:\\users\\hhc0025\\appdata\\local\\pip\\cache\\wheels\\84\\12\\4b\\15386930dab17a565fc4929c9418614e4007e88de2471df2a4\n",
      "Successfully built blabla\n",
      "Installing collected packages: tqdm, stanza, jsonpickle, anytree, ipython, pandas, blabla\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.50.2\n",
      "    Uninstalling tqdm-4.50.2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tqdm-4.50.2.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 lexico_semantic_feature\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "from blabla.utils.global_params import *\n",
    "from blabla.utils import *\n",
    "\n",
    "\n",
    "def num_demonstratives(stanza_doc):\n",
    "    \"\"\"The number of demonstravives\n",
    "            Args:\n",
    "                stanza_doc (nltk.Tree): The dependency parse tree\n",
    "            Returns:\n",
    "                (int): the number of demonstravives\n",
    "        \"\"\"\n",
    "    return len([1 for word in stanza_doc.sentences[0].words if ((word.feats is not None) and ('PronType=Dem' in word.feats))])\n",
    "\n",
    "\n",
    "def num_unique_words(stanza_doc):\n",
    "    \"\"\"Returns the number of unique words\n",
    "        Args:\n",
    "            stanza_doc (nltk.Tree): The dependency parse tree\n",
    "        Returns:\n",
    "            number of unique words \n",
    "    \"\"\"\n",
    "    return len(set([word.text for word in stanza_doc.sentences[0].words]))\n",
    "\n",
    "\n",
    "def num_word_types(stanza_doc):\n",
    "    \"\"\"Returns the number of word types\n",
    "        Args:\n",
    "            stanza_doc (nltk.Tree): The dependency parse tree\n",
    "        Returns:\n",
    "            number of word types\n",
    "    \"\"\"\n",
    "    return len(set([word.lemma for word in stanza_doc.sentences[0].words]))\n",
    "\n",
    "\n",
    "def compute_mean_word_length(stanza_doc):\n",
    "    \"\"\"Returns the mean word length\n",
    "            Args:\n",
    "                stanza_doc (nltk.Tree): The dependency parse tree\n",
    "                \n",
    "            Returns:\n",
    "                mean length of all words in the sentence\n",
    "        \"\"\"\n",
    "    return np.mean([len(word.text) for word in stanza_doc.sentences[0].words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-3 morpho_syntactic_feature\n",
    "\n",
    "from blabla.utils.global_params import *\n",
    "\n",
    "\n",
    "def num_inflected_verbs(stanza_doc):\n",
    "    \"\"\"Returns the number of inflected verbs\n",
    "\t\tArgs:\n",
    "\t\t\tNone\n",
    "\t\tReturns\n",
    "\t\t\tnum_inflected_verbs (int): The number of inflected verbs\n",
    "\t\"\"\"\n",
    "    num_inflected_verbs = 0\n",
    "    words = stanza_doc.sentences[0].words\n",
    "    for word_idx in range(len(words) - 1):\n",
    "        if (words[word_idx].pos == VERB) and (\n",
    "            words[word_idx].text != words[word_idx].lemma\n",
    "        ):\n",
    "            num_inflected_verbs += 1\n",
    "    return num_inflected_verbs\n",
    "\n",
    "\n",
    "def num_gerund_verbs(stanza_doc):\n",
    "    \"\"\"The number of gerund verbs\n",
    "\t\tArgs:\n",
    "\t\t\tNone\n",
    "\t\tReturns:\n",
    "\t\t\tnum_gerunds (int): the number of gerund verbs\n",
    "\t\"\"\"\n",
    "    num_gerunds = 0\n",
    "    for word in stanza_doc.sentences[0].words:\n",
    "        if word.feats is not None:\n",
    "            if (word.pos == VERB) and ('VerbForm=Ger' in word.feats):\n",
    "                num_gerunds += 1\n",
    "    return num_gerunds\n",
    "\n",
    "\n",
    "def num_participle_verbs(stanza_doc):\n",
    "    \"\"\"The number of participle verbs\n",
    "\t\tArgs:\n",
    "\t\t\tNone\n",
    "\t\tReturns:\n",
    "\t\t\tnum_participle_verbs (int): the number of participle verbs\n",
    "\t\"\"\"\n",
    "    num_participle_verbs = 0\n",
    "    for word in stanza_doc.sentences[0].words:\n",
    "        if word.feats is not None:\n",
    "            if (word.pos == VERB) and (\n",
    "                'VerbForm=Part' in word.feats\n",
    "            ):  # what if there are multiple words next to each other with Part?\n",
    "                num_participle_verbs += 1\n",
    "    return num_participle_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-4 pos_tag_counting\n",
    "\n",
    "import stanza\n",
    "\n",
    "\n",
    "class PosTagCounter(object):\n",
    "    \"\"\"The class that counts the number of pos tags of various types in a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stanza_doc):\n",
    "        \"\"\"The initialization method that take a dependency parse tree as input\n",
    "            Args:\n",
    "                stanza_doc (nltk.Tree): the dependency parse tree\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        self.stanza_doc = stanza_doc\n",
    "\n",
    "    def get_pos_tag_count(self, pos_tag):\n",
    "        \"\"\"Returns the number of nouns\n",
    "            Args:\n",
    "                None\n",
    "            Returns:\n",
    "                number of nouns in the sentence\n",
    "        \"\"\"\n",
    "        return len([word for word in self.stanza_doc.sentences[0].words if (word.pos == pos_tag)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-5 syntactic_feature\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from blabla.sentence_processor.yngve_tree import YngveNode\n",
    "from blabla.sentence_processor.pos_tag_counting_engine import PosTagCounter\n",
    "from blabla.utils.exceptions import *\n",
    "import blabla.utils.settings as settings\n",
    "from blabla.utils.global_params import *\n",
    "import os\n",
    "import stanza\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "\n",
    "## Ref: http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html\n",
    "class Sentence(object):\n",
    "\t\"\"\"The class that is responsible for processing a sentence and extracting the fundamental blocks such as\n",
    "\t\tthe dependency parse tree, constituency parse tree and other low level features for aggregation later.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, language, nlp, client, raw_text, sent_json=None):\n",
    "\t\t\"\"\"The initialization method for the Sentence class\n",
    "\t\tArgs:\n",
    "\t\t\tlanguage (str): The language of the raw_text\n",
    "\t\t\tnlp (obj): The Stanza NLP object.\n",
    "\t\t\tclient (obj): The CoreNLP client object.\n",
    "\t\t\traw_text (str): The raw text \n",
    "\t\t\tsent_json (JSON): The json string of a sentence if the input is a JSON\n",
    "\t\tReturns:\n",
    "\t\t\tNone:\n",
    "\t\t\"\"\"\n",
    "\t\tself.lang = language\n",
    "\t\tself._pos_tag_counter = None\n",
    "\t\tself._sent = raw_text\n",
    "\t\tself._stanza_doc = None\n",
    "\t\tself._const_pt = None\n",
    "\t\tself._yngve_tree_root = None\n",
    "\t\tself.nlp = nlp\n",
    "\t\tself.client = client\n",
    "\t\tself.json = sent_json\n",
    "\n",
    "\t@property\n",
    "\tdef pos_tag_counter(self):\n",
    "\t\t\"\"\"A property method to return the part of speech counter object\"\"\"\n",
    "\t\treturn self._pos_tag_counter\n",
    "\n",
    "\t@property\n",
    "\tdef sent(self):\n",
    "\t\t\"\"\"A property method to return the raw text of the current sentence object\"\"\"\n",
    "\t\treturn self._sent\n",
    "\n",
    "\t@property\n",
    "\tdef stanza_doc(self):\n",
    "\t\t\"\"\"A property method to return the Stanza object\"\"\"\n",
    "\t\treturn self._stanza_doc\n",
    "\n",
    "\t@property\n",
    "\tdef yngve_tree_root(self):\n",
    "\t\t\"\"\"A property method to return the root node of the Yngve tree\"\"\"\n",
    "\t\treturn self._yngve_tree_root\n",
    "\n",
    "\t@property\n",
    "\tdef const_pt(self):\n",
    "\t\t\"\"\"A property method to return the constituency parse tree\"\"\"\n",
    "\t\treturn self._const_pt\n",
    "\n",
    "\t@property\n",
    "\tdef tot_num_characters(self):\n",
    "\t\t\"\"\"A property method to return the total number of characters in the current sentence\"\"\"\n",
    "\t\treturn math.sum([len(word) for word in self._stanza_doc.sentences[0].words])\n",
    "\n",
    "\t@property\n",
    "\tdef speech_time(self):\n",
    "\t\t\"\"\"A property method to return the speech time\"\"\"\n",
    "\t\treturn math.fsum([word['duration'] for word in self.json['words']])\n",
    "\n",
    "\t@property\n",
    "\tdef start_time(self):\n",
    "\t\t\"\"\"A property method to return the start time of the current sentence\"\"\"\n",
    "\t\treturn self.json['words'][0]['start_time']\n",
    "\n",
    "\t@property\n",
    "\tdef end_time(self):\n",
    "\t\t\"\"\"A property method to return the end time of the current sentence\"\"\"\n",
    "\t\treturn self.json['words'][-1]['start_time'] + self.json['words'][-1]['duration']\n",
    "\n",
    "\t@property\n",
    "\tdef locution_time(self):\n",
    "\t\t\"\"\"A property method to return the locution time of the current sentence\"\"\"\n",
    "\t\tstart_time = self.start_time\n",
    "\t\tend_time = self.end_time\n",
    "\t\treturn end_time - start_time\n",
    "\n",
    "\t@property\n",
    "\tdef words_per_min(self):\n",
    "\t\t\"\"\"A property method to return the words per min for the current sentence\"\"\"\n",
    "\t\treturn (self.num_words() / self.locution_time()) * 60.0\n",
    "\n",
    "\tdef _get_gaps(self):\n",
    "\t\tgaps = []\n",
    "\t\tfor prev_word, next_word in zip(self.json['words'][:-1], self.json['words'][1:]):\n",
    "\t\t\tprev_end_time = prev_word['start_time'] + prev_word['duration']\n",
    "\t\t\tgaps.append(next_word['start_time'] - prev_end_time)\n",
    "\t\treturn gaps\n",
    "\n",
    "\tdef num_pauses(self, pause_duration):\n",
    "\t\t\"\"\"A method to calculate the number of pauses in the current sentence\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tpause_duration (float): The value for the duration of the pause\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tint: The number of pauses\n",
    "\t\t\"\"\"\n",
    "\t\tgaps = self._get_gaps()     \n",
    "\t\treturn sum(gap > pause_duration for gap in gaps)\n",
    "\n",
    "\tdef tot_pause_time(self, pause_duration):\n",
    "\t\t\"\"\"A method to calculate the total pause time in the current sentence\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tpause_duration (float): The value for the duration of the pause\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tfloat: The total pause time of the sentence\n",
    "\t\t\"\"\"\n",
    "\t\treturn sum([gap for gap in self._get_gaps()  if gap > pause_duration])\n",
    "\n",
    "\tdef analyze_text(self):\n",
    "\t\t\"\"\"A method to get the Stanza document\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone: \n",
    "\t\t\tReturns:\n",
    "\t\t\t\tdoc (Stanza): The Stanza document object\n",
    "\t\t\"\"\"\n",
    "\t\tdoc = self.nlp(self._sent)\n",
    "\t\treturn doc\n",
    "\n",
    "\tdef const_parse_tree(self):\n",
    "\t\t\"\"\"A method to get the constituency parse tree\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone: \n",
    "\t\t\tReturns:\n",
    "\t\t\t\tparseTree (CoreNLP): The CoreNLP constituency parse tree object\n",
    "\t\t\"\"\"\n",
    "\t\tdocument = self.client.annotate(self._sent)\n",
    "\t\tdocument = json.loads(document.text)\n",
    "\t\tpt = document[\"sentences\"][0][\"parse\"]\n",
    "\t\tparseTree = Tree.fromstring(pt)\n",
    "\t\treturn parseTree\n",
    "\n",
    "\tdef num_words(self):\n",
    "\t\t\"\"\"A method to get the number of words\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone: \n",
    "\t\t\tReturns:\n",
    "\t\t\t\tint: The number of words in the current sentence\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self._stanza_doc.sentences[0].words)\n",
    "\n",
    "\tdef _navigate_and_score_leaves(self, yngve_node, score_so_far):\n",
    "\t\t\"\"\"A method to assign Yngve scores to the leaf nodes\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone: \n",
    "\t\t\tReturns:\n",
    "\t\t\t\tYngve_Tree: The Yngve tree with the scores for the leaf nodes\n",
    "\t\t\"\"\"\n",
    "\t\tif len(yngve_node.children) == 0:\n",
    "\t\t\tyngve_node.score = score_so_far\n",
    "\t\telse:  # it has child nodes\n",
    "\t\t\tfor child in yngve_node.children:\n",
    "\t\t\t\tself._navigate_and_score_leaves(child, score_so_far + child.score)\n",
    "\n",
    "\tdef _traverse_and_build_yngve_tree(self, start_node, parent_node):\n",
    "\t\t\"\"\"A method to construct the Yngve tree\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone: \n",
    "\t\t\tReturns:\n",
    "\t\t\t\tYngve_Tree: The Yngve tree constructed from the constituency parse tree\n",
    "\t\t\"\"\"\n",
    "\t\tscore = 0\n",
    "\t\tfor child in start_node[::-1]:\n",
    "\t\t\tif isinstance(child, str):\n",
    "\t\t\t\tcurr_node = YngveNode(child, 0, parent_node)\n",
    "\t\t\telif isinstance(child, nltk.tree.Tree):\n",
    "\t\t\t\tcurr_node = YngveNode(child.label(), score, parent_node)\n",
    "\t\t\t\tscore += 1\n",
    "\t\t\t\tself._traverse_and_build_yngve_tree(child, curr_node)\n",
    "\n",
    "\tdef yngve_tree(self):\n",
    "\t\t\"\"\"The main method to construct the Yngve tree and assign values to all leaf nodes\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tyngve_tree_root_node (YngveNode): The root node of the yngve tree\n",
    "\t\t\"\"\"\n",
    "\t\tsent_child = self._const_pt[0]\n",
    "\t\tyngve_tree_root_node = YngveNode(\"S\", 0)\n",
    "\t\tself._traverse_and_build_yngve_tree(sent_child, yngve_tree_root_node)\n",
    "\t\tself._navigate_and_score_leaves(\n",
    "\t\t\tyngve_tree_root_node, yngve_tree_root_node.score\n",
    "\t\t)\n",
    "\t\treturn yngve_tree_root_node\n",
    "\n",
    "\tdef setup_dep_pt(self):\n",
    "\t\t\"\"\"A method to construct the dependency parse tree\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tNone:\n",
    "\t\t\"\"\"\n",
    "\t\tif len(self._sent) == 0:\n",
    "\t\t\traise EmptyStringException('The input string is empty')\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tself._stanza_doc = self.analyze_text()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise DependencyParsingTreeException('Dependency parse tree set up failed')\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tself._pos_tag_counter = PosTagCounter(self._stanza_doc)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise POSTagExtractionFailedException('POS Tag counter failed')\n",
    "\n",
    "\tdef setup_const_pt(self):\n",
    "\t\t\"\"\"A method to construct the constituency parse tree\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tNone:\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tself._const_pt = self.const_parse_tree()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise ConstituencyTreeParsingException(\n",
    "\t\t\t\t'Constituency parse tree set up failed. Please check if the input format (json/string) is mentioned correctly'\n",
    "\t\t\t)\n",
    "\n",
    "\tdef setup_yngve_tree(self):\n",
    "\t\t\"\"\"A method to construct the Yngve tree\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tNone\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tNone:\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tself._yngve_tree_root = self.yngve_tree()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise YngveTreeConstructionException('Yngve tree set up failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Dependency Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the Spacy Tutorial: \n",
    "\n",
    "https://spacy.io/usage/linguistic-features#dependency-parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Word2vec features (representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Exploring Word2Vec:\n",
    "\n",
    "https://github.com/lutzhamel/word2vec-simplified/blob/master/Exploring%20Word2Vec.ipynb\n",
    "\n",
    "(2) Tutorials\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
