{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Interesting_Code/Lesson_nine_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQoW88bnJJDw"
   },
   "source": [
    "# **1. Sentiment analysis for movie reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "5ExmftRvI2vt",
    "outputId": "835443b3-41d9-4ef4-adde-ff2afaa12781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GM7f64kKFmz"
   },
   "outputs": [],
   "source": [
    "# Define the feature extractor\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfO0oPNrKTiZ"
   },
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lAanTJxkKZnm",
    "outputId": "18f749e8-0715-4042-afcf-ffdc6b8efe65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "nLfNRAzCKeOK",
    "outputId": "f7f0a537-7993-44c9-958e-5befc45bb3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.3 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      7.0 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      7.0 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      7.0 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most important features as interpreted by Naive Bayes\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAZHL_xJLGSX"
   },
   "source": [
    "# **2. Sentiment Analysis for Twitter Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KhEiIzTMTIyC",
    "outputId": "85877471-842c-46aa-dfe8-d6d9af2efcb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 28.985507246376812 %\n",
      "Negative tweets percentage: 18.840579710144926 %\n",
      "Neutral tweets percentage: 52.17391304347826 %\n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "RT @PalmerReport: Americans are dropping dead left and right, but thank god we have Donald Trump up there holding press conferences so he c‚Ä¶\n",
      "RT @blakesmustache: January 22, 2020\n",
      "\n",
      "Trump told a reporter he was not concerned about the coronavirus becoming a pandemic and said it was‚Ä¶\n",
      "After watching Tiger King, for the first time in ~4 years, I feel like I finally understand how Donald Trump got elected president.\n",
      "RT @charliekirk11: Nancy Pelosi accused Donald trump of \"fiddling\" while people died\n",
      "\n",
      "Really?\n",
      "\n",
      "In January:\n",
      "\n",
      "Pelosi was passing out gold pen‚Ä¶\n",
      "RT @nancylevine: Donald Trump doesn‚Äôt want anyone to see this ad about his coronavirus failure. Make sure everyone sees it. https://t.co/9z‚Ä¶\n",
      "@michele2435reis Sometimes I feel something resembling sympathy (more like pity) for donald trump the child b/c I c‚Ä¶ https://t.co/uJ8TVF5qqK\n",
      "RT @SAVoltolin: If you're not voting for Bernie Sanders, you're helping to re-elect Donald Trump.  Fact.\n",
      "RT @CHIZMAGA: Donald Trump donated his salary to fight the virus, Nancy Pelosi asked for a raise.\n",
      "\n",
      "This is all you really need to know.\n",
      "The Truth Of Donald Trump, VOTING AND WAR Dr Paul Teich https://t.co/kKpMCw79x2 via @YouTube\n",
      "PLEASE USE MY AMAZON LINK     STAY HEALTHY\n",
      "RT @BenjaminPDixon: How many people with security clearances knew this virus was coming and, instead of directly challenging Donald Trump t‚Ä¶\n",
      "\n",
      "\n",
      "Negative tweets:\n",
      "RT @JohnWDean: It did not take Donald Trump long to use the COVID-19 crisis he created as a means to self promote and campaign at his ‚Äúmust‚Ä¶\n",
      "The day is coming...\n",
      "\n",
      "Presidential Debate - DT: Bc you'd be in jail! - Hillary Clinton vs. Donald Trump - YouTube https://t.co/m7Sx0Ta5OK\n",
      "RT @PatsHoppedUp: - Awful hair \n",
      "- Used parents money to build business\n",
      "- On camera making comments about assaulting women\n",
      "- Numerous spouse‚Ä¶\n",
      "RT @LaylaAlisha11: Donald Trump Jr's 'must-read thread' \n",
      "All the Liberal Hack Fake Journalists are reporting Propaganda from Communist Chin‚Ä¶\n",
      "RT @realTRUMPERLAND: Board of Kennedy Center\n",
      "‚ÄúYOU‚ÄôRE FIRED‚Äù\n",
      "\n",
      "Trump replaces entire board of Kennedy Center after $25 million bailout/lay-of‚Ä¶\n",
      "RT @proxcee: Jim Acosta is FAKE NEWS.\n",
      "\n",
      "#EnemyOfThePeople https://t.co/aZZtrVF5aO\n",
      "RT @zeusFanHouse: Donald #Trump:‚ÄòSick Puppy‚Äô Nancy #Pelosi Was Focused on #Impeachment During #Coronavirus #Outbreak\n",
      "\n",
      "#Trump received accol‚Ä¶\n",
      "RT @harrysiegel: Donald Trump, who spent 2016 feuding with the Catholic Pope, just lost the sports pope\n",
      "RT @middleageriot: The media is not involved in a left wing conspiracy to make Donald Trump look stupid.\n",
      "\n",
      "When they point cameras at him, i‚Ä¶\n",
      "RT @Trey_VonDinkis: @realDonaldTrump .\n",
      "üì∫FAKE NEWS\n",
      "\n",
      " - TRUMP SMACKS DOWN NANCY-BOY JIM ACOSTA at DAILY BRIEFING\n",
      "\n",
      "Fake China Puppet Acosta st‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "\n",
    "class TwitterClient(object): \n",
    "\t''' \n",
    "\tGeneric Twitter Class for sentiment analysis. \n",
    "\t'''\n",
    "\tdef __init__(self): \n",
    "\t\t''' \n",
    "\t\tClass constructor or initialization method. \n",
    "\t\t'''\n",
    "\t\t# keys and tokens from the Twitter Dev Console \n",
    "\t\tconsumer_key = 'u7L1lnR7HN85dn1qnTFO1cegb'\n",
    "\t\tconsumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'\n",
    "\t\taccess_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO'\n",
    "\t\taccess_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce'\n",
    "\n",
    "\t\t# attempt authentication \n",
    "\t\ttry: \n",
    "\t\t\t# create OAuthHandler object \n",
    "\t\t\tself.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "\t\t\t# set access token and secret \n",
    "\t\t\tself.auth.set_access_token(access_token, access_token_secret) \n",
    "\t\t\t# create tweepy API object to fetch tweets \n",
    "\t\t\tself.api = tweepy.API(self.auth) \n",
    "\t\texcept: \n",
    "\t\t\tprint(\"Error: Authentication Failed\") \n",
    "\n",
    "\tdef clean_tweet(self, tweet): \n",
    "\t\t''' \n",
    "\t\tUtility function to clean tweet text by removing links, special characters \n",
    "\t\tusing simple regex statements. \n",
    "\t\t'''\n",
    "\t\treturn ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "\n",
    "\tdef get_tweet_sentiment(self, tweet): \n",
    "\t\t''' \n",
    "\t\tUtility function to classify sentiment of passed tweet \n",
    "\t\tusing textblob's sentiment method \n",
    "\t\t'''\n",
    "\t\t# create TextBlob object of passed tweet text \n",
    "\t\tanalysis = TextBlob(self.clean_tweet(tweet)) \n",
    "\t\t# set sentiment \n",
    "\t\tif analysis.sentiment.polarity > 0: \n",
    "\t\t\treturn 'positive'\n",
    "\t\telif analysis.sentiment.polarity == 0: \n",
    "\t\t\treturn 'neutral'\n",
    "\t\telse: \n",
    "\t\t\treturn 'negative'\n",
    "\n",
    "\tdef get_tweets(self, query, count = 10): \n",
    "\t\t''' \n",
    "\t\tMain function to fetch tweets and parse them. \n",
    "\t\t'''\n",
    "\t\t# empty list to store parsed tweets \n",
    "\t\ttweets = [] \n",
    "\n",
    "\t\ttry: \n",
    "\t\t\t# call twitter api to fetch tweets \n",
    "\t\t\tfetched_tweets = self.api.search(q = query, count = count) \n",
    "\n",
    "\t\t\t# parsing tweets one by one \n",
    "\t\t\tfor tweet in fetched_tweets: \n",
    "\t\t\t\t# empty dictionary to store required params of a tweet \n",
    "\t\t\t\tparsed_tweet = {} \n",
    "\n",
    "\t\t\t\t# saving text of tweet \n",
    "\t\t\t\tparsed_tweet['text'] = tweet.text \n",
    "\t\t\t\t# saving sentiment of tweet \n",
    "\t\t\t\tparsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "\n",
    "\t\t\t\t# appending parsed tweet to tweets list \n",
    "\t\t\t\tif tweet.retweet_count > 0: \n",
    "\t\t\t\t\t# if tweet has retweets, ensure that it is appended only once \n",
    "\t\t\t\t\tif parsed_tweet not in tweets: \n",
    "\t\t\t\t\t\ttweets.append(parsed_tweet) \n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\ttweets.append(parsed_tweet) \n",
    "\n",
    "\t\t\t# return parsed tweets \n",
    "\t\t\treturn tweets \n",
    "\n",
    "\t\texcept tweepy.TweepError as e: \n",
    "\t\t\t# print error (if any) \n",
    "\t\t\tprint(\"Error : \" + str(e)) \n",
    "\n",
    "def main(): \n",
    "\t# creating object of TwitterClient Class \n",
    "\tapi = TwitterClient() \n",
    "\t# calling function to get tweets \n",
    "\ttweets = api.get_tweets(query = 'Donald Trump', count = 200) \n",
    "\n",
    "\t# picking positive tweets from tweets \n",
    "\tptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "\t# percentage of positive tweets \n",
    "\tprint(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "\t# picking negative tweets from tweets \n",
    "\tntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "\t# percentage of negative tweets \n",
    "\tprint(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "\t# percentage of neutral tweets \n",
    "\tprint(\"Neutral tweets percentage: {} %\".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets))) \n",
    "\n",
    "\t# printing first 5 positive tweets \n",
    "\tprint(\"\\n\\nPositive tweets:\") \n",
    "\tfor tweet in ptweets[:10]: \n",
    "\t\tprint(tweet['text']) \n",
    "\n",
    "\t# printing first 5 negative tweets \n",
    "\tprint(\"\\n\\nNegative tweets:\") \n",
    "\tfor tweet in ntweets[:10]: \n",
    "\t\tprint(tweet['text']) \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\t# calling main function \n",
    "\tmain() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "fLBaq5Snffx5",
    "outputId": "61600784-a267-4604-aab2-c54f82cbebb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/a3/1218a3b5651dbcba1699101c84e5c84c36cbba360d9dbf29f2ff18482982/vaderSentiment-3.3.1-py2.py3-none-any.whl (125kB)\n",
      "\r",
      "\u001b[K     |‚ñà‚ñà‚ñã                             | 10kB 19.7MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 20kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 30kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 40kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 51kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 61kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 71kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 81kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 92kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 102kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 112kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 122kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 4.8MB/s \n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkg7Jpv6Zvwd"
   },
   "source": [
    "# **3. Sentiment Analysis for Amazon Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7Te93vKeLgt"
   },
   "outputs": [],
   "source": [
    "# importing all the required Libraries\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiEDSJtKhLHG"
   },
   "outputs": [],
   "source": [
    "# Data download link:\n",
    "# https://drive.google.com/drive/folders/0B4Hj2axlpCcxWldiajctWmY0NG8\n",
    "file=glob.glob('/Data/Tested_Data/ReviewSample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kfV1sGazjFS"
   },
   "outputs": [],
   "source": [
    "# Reading a multiple json files from a single json file 'ReviewSample.json'.\n",
    "review=[]\n",
    "with open(file[0]) as data_file:\n",
    "    data=data_file.read()\n",
    "    for i in data.split('\\n'):\n",
    "        review.append(i)\n",
    "        \n",
    "# Making a list of Tuples containg all the data of json files.\n",
    "reviewDataframe=[]\n",
    "for x in review:\n",
    "    try:\n",
    "        jdata=json.loads(x)\n",
    "        reviewDataframe.append((jdata['reviewerID'],jdata['asin'],jdata['reviewerName'],jdata['helpful'][0],jdata['helpful'][1],jdata['reviewText'],jdata['overall'],jdata['summary'],jdata['unixReviewTime'],jdata['reviewTime'])) \n",
    "    except:\n",
    "        pass        \n",
    "    \n",
    "# Creating a dataframe using the list of Tuples got in the previous step.    \n",
    "dataset=pd.DataFrame(reviewDataframe,columns=['Reviewer_ID','Asin','Reviewer_Name','helpful_UpVote','Total_Votes','Review_Text','Rating','Summary','Unix_Review_Time','Review_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yB0WUXmEz4DK"
   },
   "outputs": [],
   "source": [
    "# Function to calculate sentiments using Naive Bayes Analyzer\n",
    "\n",
    "def NaiveBaiyes_Sentimental(sentence):\n",
    "    blob = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())\n",
    "    NaiveBayes_SentimentScore=blob.sentiment.classification\n",
    "    return NaiveBayes_SentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuH6xQ4dz9uu"
   },
   "outputs": [],
   "source": [
    "# Function to calculate sentiments using Vader Sentiment Analyzer\n",
    "\n",
    "# VADER sentiment analysis tool for getting Compound score.\n",
    "def sentimental(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    score=vs['compound']\n",
    "    return score\n",
    "\n",
    "# VADER sentiment analysis tool for getting pos, neg and neu.\n",
    "def sentimental_Score(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    score=vs['compound']\n",
    "    if score >= 0.5:\n",
    "        return 'pos'\n",
    "    elif (score > -0.5) and (score < 0.5):\n",
    "        return 'neu'\n",
    "    elif score <= -0.5:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlbsQaCf0hxd"
   },
   "outputs": [],
   "source": [
    "# sentiment calculation by our data as input\n",
    "Selected_Rows=dataset.head(100000)\n",
    "Selected_Rows['Sentiment_Score']=Selected_Rows['Review_Text'].apply(lambda x: sentimental_Score(x))\n",
    "pos = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'pos']\n",
    "print(pos)\n",
    "neg = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'neg']\n",
    "print(neg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM23fNTU1x72Ivjt3/yWm+u",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lesson_nine_examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
