{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAAGG8HN22rW"
      },
      "source": [
        "# SIKS Assignment\n",
        "__Advances in Information Retrieval__\n",
        "\n",
        "_by Djoerd Hiemstra_\n",
        "\n",
        "In this assignment you will learn how to calculate evaluation measures.\n",
        "\n",
        "## Background information\n",
        "\n",
        "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, [Chapter 8, Evaluation in information retrieval](http://nlp.stanford.edu/IR-book/pdf/08eval.pdf), Cambridge University Press. 2008\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YixpVlGk22ra"
      },
      "source": [
        "## 1. Compute evaluation measures\n",
        "\n",
        "For a TREC-style (TREC is the Text Retrieval Conference) scientific evaluation of a search engine, we make a _run file_ that contains for each test query the top documents retrieved. We then have to test if the run file contains the right answers for each query, and if so, at which rank. The right answers are called _\"relevance judgments\"_ in TREC, because they result from human inspection (judgments) of the retrieved results. Suppose the set of relevant documents (the document idenifiers) is called `relevant`, then we might define those as follows in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1S7a_PY22rb"
      },
      "outputs": [],
      "source": [
        "relevant = set([2, 3, 5, 8, 13, 21])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze-myEaF22rc"
      },
      "source": [
        "A perfect run would retrieve exactly these 6 documents in any order. Now, suppose the list of retrieved documents (the document identifiers) is called: `retrieved`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMOfw8qA22rd"
      },
      "outputs": [],
      "source": [
        "retrieved = [4, 2, 18, 16, 8, 46, 32, 22, 47, 39, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16pW0z-K22rd"
      },
      "source": [
        "One of the simplest evaluation measures we can think of is the _Success at rank 1_. The measure answers the question: Was the first document retrieved a relevant document? _Success at rank 1_ returns 1 if the first document is relevant, and 0 otherwise. A possible implementation is: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb6a18FL22re"
      },
      "outputs": [],
      "source": [
        "def success_at_1 (relevant, retrieved):\n",
        "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "success_at_1(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyk5nro422re"
      },
      "source": [
        "The first retrieved documentid is 4 which is not in the set of relevant documents, so the score is 0.\n",
        "\n",
        "Note how easy it is to check if an item occurs in a Python set or list by using the keyword: `in`. Similarly, you can loop over all items in a set of list with: `for doc in retrieved:`, where doc will refer to each item in the set or list. Be sure to use the internet to brush up your knowledge on Python constructs, for instance on [Python list slicing](https://duckduckgo.com/?q=python+list+slicing). Also note that the code above checks if at least one document is retrieved to avoid an index out of bounds exception."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNUj-GXQ22rf"
      },
      "source": [
        "### Exercise A, Success at 5\n",
        "The measure _Success at 5_ returns 1 if a relevant document is among the first 5 documents retrieved and zero otherwise. Implement _Success at 5_ below.\n",
        "\n",
        "> Success at X measures are well-suited in cases where there is typically only one relevant document.\n",
        "> This is for instance the case in our product search scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrPk3tqF22rg"
      },
      "outputs": [],
      "source": [
        "def success_at_5(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "success_at_5(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-z-Q1mt22rh"
      },
      "source": [
        "Similarly implement success at rank 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcOj0LnR22ri"
      },
      "outputs": [],
      "source": [
        "def success_at_10(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "success_at_10(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7szOvyTr22rj"
      },
      "source": [
        "### Exercise B, number of documents retrieved\n",
        "\n",
        "Give the function that measures the number of documents retrieved.\n",
        "\n",
        "> Tip: compute the length of the list `retrieved`. \n",
        ">\n",
        "> You do not need to touch `relevant`, but we leave the parameter here so\n",
        "> all measures have the same function signature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-24fSQ622rk"
      },
      "outputs": [],
      "source": [
        "def num_retrieved (relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "num_retrieved(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iewA7gcJ22rl"
      },
      "source": [
        "### Exercise C, Precision\n",
        "\n",
        "Implement _Precision_ using Formula 8.1 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9BlYOf222rl"
      },
      "outputs": [],
      "source": [
        "def precision(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "precision(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33NGkSW22rn"
      },
      "source": [
        "### Exercise D, Recall\n",
        "\n",
        "Implement _Recall_ using Formula 8.2 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlmBhzIw22rn"
      },
      "outputs": [],
      "source": [
        "def recall(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "recall(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9srAB3hf22ro"
      },
      "source": [
        "### Exercise E, F-measure\n",
        "\n",
        "The balanced F measure (_F_ with β=1) is defined as the harmonic mean of precision and\n",
        "recall. Implement _F_ using Formula 8.6 from [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).\n",
        "\n",
        "> Tip: reuse your implementations of precision and recall\n",
        "\n",
        "> What is the advantage of using the harmonic mean rather than \"averaging\" (using the arithmetic mean)? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t22UG1F222ro"
      },
      "outputs": [],
      "source": [
        "def f_measure(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "f_measure(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcGlhwF22rp"
      },
      "source": [
        "### Exercise F,  Precision at rank X\n",
        "\n",
        "Precision, Recall and F are _set_-based measures, but our search system returns a ranked _list_ of results. One way to address this is to measure precision for several cut-off levels _X_ in the ranked list. We did this before with the _Success at rank 5_ measure for _X_=5.\n",
        "\n",
        "Implement the function `precision_at_rank_X()` that measures the precision at rank _X_ below. \n",
        "\n",
        "> Interesting fact: For _X_=1, the _Precision at rank 1_ would be the samen as _Success at rank 1_ (why?) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moSfLurm22rp"
      },
      "outputs": [],
      "source": [
        "def precision_at_rank_X(relevant, retrieved, X):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "precision_at_rank_X(relevant, retrieved, X=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9x_PPMu22rq"
      },
      "source": [
        "### Exercise G, R-Precision\n",
        "Implement R-Precision as defined on Page 161 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA4HxCAZ22rq"
      },
      "outputs": [],
      "source": [
        "def r_precision (relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "r_precision(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbMdBxXN22rr"
      },
      "source": [
        "### Exercise H,  Interpolated precision at _recall_ X\n",
        "\n",
        "Another way to address ranked retrieval is to measure precision for several _recall_ levels _X_.\n",
        "\n",
        "Implement the function `interpolated_precision_at_rank_X()` that measures the interpolated precision at rank _X_ as defined by Formula 8.7 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book).\n",
        "\n",
        "> Tip: calculate for each rank the recall. If the recall is greater than or equal to X, \n",
        "> calculate the precision. Keep the highest (maximum) precision of those to be returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo4IR8o_22rr"
      },
      "outputs": [],
      "source": [
        "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    \n",
        "interpolated_precision_at_recall_X(relevant, retrieved, X=0.1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9o0b0TZ22rr"
      },
      "source": [
        "### Exercise I, Average Precision\n",
        "\n",
        "For a single information need, _Average Precision_ is the average of the precision value obtained for the set of top k documents existing after each relevant document is retrieved (see [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book), Pages 159 and 160). Implement _Average Precision_ for a single information need. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Pm1lVl22rs"
      },
      "outputs": [],
      "source": [
        "def average_precision(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "\n",
        "average_precision(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsrLNA_N22rs"
      },
      "source": [
        "### Exercise J, Reciprocal rank\n",
        "\n",
        "Like the _Success at X_ meaure, the [reciprocal rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) is a measure that is well-suited for cases in which there is only 1 relevant results for each query. If the relevant document is found at rank _r_, then the reciprocal rank is defined as: 1 / _r_. If the relevant document is not found, it is 0.\n",
        "\n",
        "Implement the function `reciprocal_rank()` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdLGGOG422rs"
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank(relevant, retrieved):\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "\n",
        "reciprocal_rank(relevant, retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7xUCAzO22rt"
      },
      "source": [
        "## 2. Measures in TREC \n",
        "\n",
        "The relevance judgments are provided by TREC in so-called _\"qrels\"_ files that look as follows:\n",
        "\n",
        "    1000 Q0 1341 1\n",
        "    1000 Q0 1231 0\n",
        "    1001 Q0 12332 1\n",
        "     ...\n",
        "\n",
        "Here, the first column is the query identifier, the second column is the query number within that topic. This is currently unused and should always be Q0. The third column is the document identifier that was examined by the judges. The fourth column is the relevance of the document, where 0 means the document was not relevant and a value larger than 0 means the document was relevant.\n",
        "\n",
        "Below we provide some Python code that reads the _qrels_ and the _run_. The qrels will be put in the Python dictionary `all_relevant`. A [Python dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) provides quick lookup of values given a key. We will use the `query_id` as a key, and a [set](https://docs.python.org/3/tutorial/datastructures.html#sets) of relevant document identifiers. For the partial qrels file above, `all_relevant` would look as follows:\n",
        "\n",
        "    {\n",
        "        \"1000\": set([\"1341\", \"1231\"]),\n",
        "        \"1001\": set([\"12332\"])\n",
        "    }\n",
        "    \n",
        "We will use a dictionary called `all_retrieved` with `query_id` as key, and as value a [Python list](https://docs.python.org/3/tutorial/introduction.html#lists) of document identifiers retrieved by the IR system:\n",
        "\n",
        "    {\n",
        "        \"1000\": [\"1341\", \"12346, \"2345\"],\n",
        "        \"1001\": [..., ..., ...],\n",
        "        ...\n",
        "    }\n",
        "    \n",
        "Please examine the code below, and make sure you understand every line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaveQWHh22rt"
      },
      "outputs": [],
      "source": [
        "def read_qrels_file(qrels_file):\n",
        "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
        "    with open(qrels_file, 'r') as qrels:\n",
        "        for line in qrels:\n",
        "            (qid, q0, doc_id, rel) = line.strip().split()\n",
        "            if qid not in trec_relevant:\n",
        "                trec_relevant[qid] = set()\n",
        "            if (rel != \"0\"):\n",
        "                trec_relevant[qid].add(doc_id)\n",
        "    return trec_relevant\n",
        "\n",
        "def read_run_file(run_file):\n",
        "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
        "    with open(run_file, 'r') as run:\n",
        "        for line in run:\n",
        "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
        "            if qid not in trec_retrieved:\n",
        "                trec_retrieved[qid] = []\n",
        "            trec_retrieved[qid].append(doc_id) \n",
        "    return trec_retrieved\n",
        "    \n",
        "\n",
        "def read_eval_files(qrels_file, run_file):\n",
        "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
        "\n",
        "(all_relevant, all_retrieved) = read_eval_files('wapo2018.qrels', 'wapo2018baseline.run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEytB2mk22rt"
      },
      "source": [
        "### Exercise K (number of queries)\n",
        "\n",
        "Give Python code that counts the number of queries in the file `wapo2018baseline.run` (use the result from the cell above). Are there results for all queries?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MafxD3Dr22ru"
      },
      "outputs": [],
      "source": [
        "# BEGIN ANSWER\n",
        "# END ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFGPDAVv22ru"
      },
      "source": [
        "### Exercise L (number of retrieved per query)\n",
        "\n",
        "Give the code that counts for each query in your baseline run the number of documents that were retrieved for that query (use `print()` to print the result for each `query_id`). Did the search system find 1000 documents for each query?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21N7qbff22ru"
      },
      "outputs": [],
      "source": [
        "# BEGIN ANSWER\n",
        "# END ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPESeZIT22rv"
      },
      "source": [
        "Below you find two functions that take `all_relevant` and `all_retrieved` to compute the mean result. The first function, `mean()` uses the same for loop as your function for Exercise L (If not: check Exercise L again). It computes the mean value over all queries. The function `mean()`'s first function argument, `measure`, is a special argument: it is a function too! The `mean` function sums the total score for the measure and divides it by the number of queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuk7Na622rv"
      },
      "outputs": [],
      "source": [
        "def mean(measure, all_relevant, all_retrieved):\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for qid in all_relevant:\n",
        "        relevant  = all_relevant[qid]\n",
        "        if (len(relevant) > 0):\n",
        "            retrieved = all_retrieved.get(qid, [])\n",
        "            value = measure(relevant, retrieved)\n",
        "            total += value\n",
        "            count += 1\n",
        "    return \"mean \" + measure.__name__, total / count\n",
        "\n",
        "mean(average_precision, all_relevant, all_retrieved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIOuxDRz22rv"
      },
      "source": [
        "### Exercise M (check your evaluation results)\n",
        "\n",
        "The following two functions use your implementation of the metrics to create an evaluation overview, and prints the results for the TREC qrels file `wapo2018.qrels` and the baseline run `wapo2018baseline.run`. \n",
        "The baseline run was made with [Anserini](https://github.com/castorini/anserini) using BM25 term weighting.\n",
        "Complete this assignment by checking your evaluation metrics with ones reported by the Anserini [Regressions for the Washington Post (Core18)](https://github.com/castorini/anserini/blob/master/docs/regressions-core18.md#effectiveness). Are your results the same for average precision and precision at rank 30?\n",
        "\n",
        "> Tip: Ask the SIKS course directors for the correctness of your other measures\n",
        "\n",
        "> For Google Colab you need to add the following code: `from google.colab import files` and `files.upload()`\n",
        "> and upload the files [wapo2018.qrels](https://www.cs.ru.nl/~hiemstra/siks/wapo2018.qrels) and \n",
        "> [wapo2018baseline.run](https://www.cs.ru.nl/~hiemstra/siks/wapo2018baseline.run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_O1M__r22rv"
      },
      "outputs": [],
      "source": [
        "def trec_eval(qrels_file, run_file):\n",
        "\n",
        "    def precision_at_rank_1(rel, ret): return precision_at_rank_X(rel, ret, X=1)\n",
        "    def precision_at_rank_5(rel, ret): return precision_at_rank_X(rel, ret, X=5)\n",
        "    def precision_at_rank_10(rel, ret): return precision_at_rank_X(rel, ret, X=10)\n",
        "    def precision_at_rank_30(rel, ret): return precision_at_rank_X(rel, ret, X=30)\n",
        "    def precision_at_rank_50(rel, ret): return precision_at_rank_X(rel, ret, X=50)\n",
        "    def precision_at_rank_100(rel, ret): return precision_at_rank_X(rel, ret, X=100)\n",
        "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
        "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
        "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
        "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
        "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
        "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
        "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
        "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
        "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
        "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
        "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
        "\n",
        "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
        "    \n",
        "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
        "    if len(unknown_qids) > 0:\n",
        "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
        "\n",
        "    metrics = [num_retrieved,\n",
        "               success_at_1,\n",
        "               success_at_5,\n",
        "               success_at_10,\n",
        "               r_precision,\n",
        "               precision_at_rank_1,\n",
        "               precision_at_rank_5,\n",
        "               precision_at_rank_10,\n",
        "               precision_at_rank_30,\n",
        "               precision_at_rank_50,\n",
        "               precision_at_rank_100,\n",
        "               precision_at_recall_00,\n",
        "               precision_at_recall_01,\n",
        "               precision_at_recall_02,\n",
        "               precision_at_recall_03,\n",
        "               precision_at_recall_04,\n",
        "               precision_at_recall_05,\n",
        "               precision_at_recall_06,\n",
        "               precision_at_recall_07,\n",
        "               precision_at_recall_08,\n",
        "               precision_at_recall_09,\n",
        "               precision_at_recall_10,\n",
        "               reciprocal_rank,\n",
        "               average_precision]\n",
        "    \n",
        "    return [mean(metric, all_relevant, all_retrieved) for metric in metrics]\n",
        "\n",
        "\n",
        "def print_trec_eval(qrels_file, run_file):\n",
        "    results = trec_eval(qrels_file, run_file)\n",
        "    print(\"Results for {}\".format(run_file))\n",
        "    for (metric, score) in results:\n",
        "        print(\"{:<30} {:.4}\".format(metric, score))\n",
        "\n",
        "print_trec_eval('wapo2018.qrels', 'wapo2018baseline.run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7IpAYVq22rw"
      },
      "source": [
        "### Exercise N, evaluate your new run\n",
        "\n",
        "Add a new run file to the notebook by clicking \"Upload\" in the Jupyter file viewer, for instance, call your new run file `wapo2018new.run`.  Then check the evaluation results of the new run.\n",
        "\n",
        "> ((For Google Colab add the following code: `from google.colab import files` and `files.upload()`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbPzQhd22rw"
      },
      "source": [
        "### Bonus Exercise O, significance testing\n",
        "\n",
        "Section 8.8 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book) mentions that the Information Retrieval community _\"increasingly demands\"_ significance testing. One of the simplest tests one can perform is the two-tailed [sign test](https://en.wikipedia.org/wiki/Sign_test).\n",
        "\n",
        "Use the baseline run `wapo2018baseline.run` and your new run `wapo2018new.run`. Compute the number of queries that increase/descrease performance (called `better`/`worse` in the code below). Use these values to compute the _p_ value of the two-tailed sign test. Is the difference between the runs significant?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lBtf8PJ22rx"
      },
      "outputs": [],
      "source": [
        "def sign_test_p_value(measure, qrels_file, run_file_1, run_file_2):\n",
        "    all_relevant = read_qrels_file(qrels_file)\n",
        "    all_retrieved_1 = read_run_file(run_file_1)\n",
        "    all_retrieved_2 = read_run_file(run_file_2)\n",
        "    better = 0\n",
        "    worse  = 0\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    return(better, worse)\n",
        "\n",
        "sign_test_p_value(average_precision, 'wapo2018.qrels', 'wapo2018baseline.run', 'wapo2018new.run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_zGRd622rx"
      },
      "source": [
        "### Bonus Exercise P, Assessing relevance\n",
        "\n",
        "Download the new qrels file [`wapo2018-alt.qrels`](http://www.cs.ru.nl/~hiemstra/siks/wapo2018-alt.qrels) and add it to the notebook by clicking \"Upload\" in the Jupyter file viewer.\n",
        "\n",
        "> (For Google Colab add the following code: `from google.colab import files` and `files.upload()`)\n",
        "\n",
        "Read Section 8.5 of [Manning, Raghavan and Schütze](http://nlp.stanford.edu/IR-book). \n",
        "Two human judges rated the relevance of the TREC documents; one is provided by the file `wapo2018.qrels`, the other by the file `wapo2018-alt.qrels`. As above, in the files: 0 = nonrelevant and >1 = relevant. Compute the kappa statitistic for the two judges as described in Section 8.5 (See Table 8.2). Is the agreement good, fair or dubious?\n",
        "\n",
        "> Tip: The kappa statistic is computed over all data (so not per query)\n",
        "> We already provided the code that constructs the contingency table.\n",
        ">\n",
        "> Tip 2: to debug, inspect the intermediate results when calculating kappa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMV-pYeQ22ry"
      },
      "outputs": [],
      "source": [
        "def read_all_qrels_file(qrels_file):\n",
        "    trec_relevant = dict()\n",
        "    with open(qrels_file, 'r') as qrels:\n",
        "        for line in qrels:\n",
        "            (qid, q0, doc_id, rel) = line.strip().split()\n",
        "            if qid not in trec_relevant:\n",
        "                trec_relevant[qid] = dict()\n",
        "            trec_relevant[qid][doc_id] = rel\n",
        "    return trec_relevant\n",
        "\n",
        "def contingency_table(qrels_file_1, qrels_file_2):\n",
        "    all_relevant_1 = read_all_qrels_file(qrels_file_1)\n",
        "    all_relevant_2 = read_all_qrels_file(qrels_file_2)\n",
        "    table = [[0, 0], [0, 0]]\n",
        "    for qid in all_relevant_1:\n",
        "        for did in all_relevant_1[qid]:\n",
        "            if (all_relevant_1[qid][did] == \"0\" and all_relevant_2[qid][did] == \"0\"):\n",
        "                table[0][0] += 1\n",
        "            elif (all_relevant_1[qid][did] == \"0\" and all_relevant_2[qid][did] != \"0\"):\n",
        "                table[0][1] += 1\n",
        "            elif (all_relevant_1[qid][did] != \"0\" and all_relevant_2[qid][did] == \"0\"):\n",
        "                table[1][0] += 1\n",
        "            elif (all_relevant_1[qid][did] != \"0\" and all_relevant_2[qid][did] != \"0\"):\n",
        "                table[1][1] += 1\n",
        "            else:\n",
        "                raise ValueError(\"Missing relevance judgments.\")\n",
        "    return table\n",
        "\n",
        "def kappa_value(qrels_file_1, qrels_file_2):\n",
        "    table = contingency_table(qrels_file_1, qrels_file_2)\n",
        "    # BEGIN ANSWER\n",
        "    # END ANSWER\n",
        "    return kappa\n",
        "\n",
        "kappa_value('wapo2018.qrels', 'wapo2018-alt.qrels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0PvXar022ry"
      },
      "source": [
        "(The correct kappa value is: 0.625)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "IReval.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}